import sqlite3
import os
from multiprocessing.connection import answer_challenge

from flask import render_template
from pydantic import BaseModel, Field, create_model

from utils import DB_PATH, get_references, TypeOfStudy
from pdfs import test_if_pdf_exists
from AI_utils import *


class ROB_item(BaseModel):
    domain: str = Field(description="the name of the bias domain according to the ROB2.0 tool")
    risk_of_bias: str = Field(description="level of risk of bias on this dimension: low, some concern, high")
    justification: str = Field(description="justification leading to this risk of bias ranking on this domain")

class ROB(BaseModel):
    r: list[ROB_item] = Field(description="list of dimension of the risk of bias according ROB2.0")





def build_ROB_RCT_prompt():

    rob_user_prompt = """"
    Here is an article reporting a randomized clinical trial in the following context.
    using the ROB2.0 tools, assess and justify the risk of bias on the five ROB2.0 key domain:
     - Bias arising from the randomization process
     - Bias due to deviations from intended interventions
     - Bias due to missing outcome data
     - Bias in measurement of the outcome
     - Bias in selection of the reported result
    
     CONTEXT:
     {context}
    """

    system_prompt = """
        You are a specialist in randomized clinical trials and systematic reviews.
        Extract information using only the given context and does not used your memory or your knowledge of the concerned trial.
        """

    return rob_user_prompt, system_prompt

def build_ROB_DIAG_prompt():

    rob_user_prompt = """"
    Here is an article reporting a diagnostic accuracy study in the following context.
    Using the QUADAS-2 tools, assess and justify the risk of bias on the seven QUADAS-2 key domain:   
     - Bias patient selection
     - Applicability patient selection
     - Bias index test(s)
     - Applicability index test(s)
     - Bias in reference standard
     - Applicability reference standard
     - Bias flow and timing
    
     CONTEXT:
     {context}
    """

    system_prompt = """
        You are a specialist in diagnostic accuracy studies and systematic reviews.
        Extract information using only the given context and does not used your memory or your knowledge of the concerned study.
        """

    return rob_user_prompt, system_prompt

def build_ROB_prompt(study_type):
    match study_type:
        case TypeOfStudy.DIAG.value:
            rob_user_prompt, system_prompt = build_ROB_DIAG_prompt()
        case _:
            rob_user_prompt, system_prompt = build_ROB_RCT_prompt()

    return rob_user_prompt, system_prompt


def AI_ROB(study_id, record_id, project_id, llm_name):
    sql = "SELECT type_of_study FROM projects WHERE id=?"
    study_type = sql_select_fetchone(sql, (project_id,))['type_of_study']

    references = get_references(study_id)
    pdf_exists = test_if_pdf_exists(record_id)
    if not pdf_exists: return None

    user_prompt, system_prompt = build_ROB_prompt(study_type)
    context = get_pdf(record_id)
    rob = invoke_llm_structured_output(system_prompt, user_prompt, context, ROB)
    rob = rob.r

    data = dict()
    i = 1
    for e in rob:
        risk_of_bias = e.risk_of_bias.lower()
        level = 0
        if "low" in risk_of_bias: level=1
        if "some concern" in risk_of_bias: level=2
        if "high" in risk_of_bias: level=3

        justification = e.justification + "\ngenerated by "+ llm_name + " model"

        data[i]={'domain': e.domain, 'justification': justification, 'risk_of_bias': risk_of_bias, 'level':level}
        i += 1

    return data



def AI_check_ROB_RCT(extracted_data, record_id):
    context = get_pdf(record_id)

    prompt_system = "Your are an expert of clinical trials and systematic reviews."

    prompt = "Given the randomized clinical trial described in the CONTEXT below, could you check the correctness of this risk of bias assessment.\n" \
             "Please answer for each item with 'OK' if the item is correct, or 'ERROR' if it is not correct. If it is not correct, please provide the correct information for this item.\n"
    prompt += "RISK OF BIAS ASSESSMENT:\n" + extracted_data + "\n"
    prompt += "CONTEXT:\n" + context + "\n"

    answer = invoke_llm_text_output("secondary", prompt, prompt_system)

    return answer

def AI_check_ROB_DIAG(extracted_data, record_id):
    context = get_pdf(record_id)

    prompt_system = "Your are an expert of diagnostic accuracy studies and systematic reviews."

    prompt = "Given the diagnostic accuracy study described in the CONTEXT below, could you check the correctness of this risk of bias assessment.\n" \
             "Please answer for each item with 'OK' if the item is correct, or 'ERROR' if it is not correct. If it is not correct, please provide the correct information for this item.\n"
    prompt += "RISK OF BIAS ASSESSMENT:\n" + extracted_data + "\n"
    prompt += "CONTEXT:\n" + context + "\n"

    answer = invoke_llm_text_output("secondary", prompt, prompt_system)

    return answer

def AI_check_ROB(extracted_data, record_id, project_id):
    sql = "SELECT type_of_study FROM projects WHERE id=?"
    study_type = sql_select_fetchone(sql, (project_id,))['type_of_study']
    match study_type:
        case TypeOfStudy.DIAG.value:
            answer = AI_check_ROB_DIAG(extracted_data, record_id)
        case _:
            answer = AI_check_ROB_RCT(extracted_data, record_id)

    return answer




def get_AI_data_ROB(study_id, record_id, project_id):
    AI_data = AI_ROB(study_id, record_id, project_id, current_app.config['LLM_NAME'])
    return AI_data



